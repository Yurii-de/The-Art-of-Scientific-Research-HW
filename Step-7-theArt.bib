@inproceedings{chase2022langchain,
  title={LangChain: A Framework for Developing Applications Powered by Language Models},
  author={Chase, Harrison},
  year={2022},
  url={https://github.com/hwchase17/langchain},
  abstract={LangChain is a framework designed to simplify the development of applications using language models. It provides a modular approach to integrate language models with external data sources and allows for easy construction of RAG pipelines.}
}

@article{deepset2020haystack,
  title={Haystack: End-to-End Neural Question Answering at Scale},
  author={deepset},
  year={2020},
  url={https://www.datenworks.com.br/wp-content/uploads/2021/09/haystack.pdf},
  abstract={Haystack is an open-source framework that enables developers to build scalable question-answering systems, retrieval-based pipelines, and RAG-based chatbots. It supports various components for document retrieval, indexing, and answer generation, facilitating customization and scalability.}
}

@inproceedings{lewis2020retrieval,
  title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and et al.},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020},
  abstract={We present Retrieval-Augmented Generation (RAG), a new architecture for knowledge-intensive NLP tasks that combines pre-trained parametric and non-parametric memory. RAG models leverage pre-trained sequence-to-sequence models as generators and neural retrievers to access external knowledge, achieving state-of-the-art results on various tasks.}
}

@article{ren2021deepspeed,
  title={DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
  author={Ren, Shaojie and Kumar, Shaden and Zhang, Minjia and others},
  journal={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={1--14},
  year={2021},
  doi={10.1145/3437801.3441686},
  abstract={DeepSpeed is a deep learning optimization library that makes distributed training easy, efficient, and effective. It enables training models with over 100 billion parameters, offering system optimizations that reduce memory consumption and increase training speed.}
}

@misc{openai2023retrievalplugin,
  title={Retrieval Plugin for ChatGPT},
  author={OpenAI},
  year={2023},
  url={https://platform.openai.com/docs/plugins/retrieval},
  abstract={The Retrieval Plugin enables ChatGPT to access personal or organizational information by connecting it to proprietary data sources. It allows developers to augment ChatGPT's responses with relevant documents, supporting the RAG approach for more accurate and context-aware outputs.}
}

@misc{onnxruntime2022kubernetes,
  title={Scaling Machine Learning Inference with ONNX Runtime and Kubernetes},
  author={Microsoft},
  year={2022},
  url={https://azure.microsoft.com/en-us/blog/scaling-machine-learning-inference-with-onnx-runtime-and-kubernetes/},
  abstract={This resource discusses how to use ONNX Runtime in conjunction with Kubernetes to scale machine learning inference workloads. It highlights optimization techniques for deploying models efficiently in production environments.}
}

@article{karpukhin2020dense,
  title={Dense Passage Retrieval for Open-Domain Question Answering},
  author={Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6769--6781},
  year={2020},
  doi={10.18653/v1/2020.emnlp-main.550},
  abstract={We propose Dense Passage Retrieval (DPR), a new method for open-domain question answering that efficiently retrieves relevant passages from a large corpus. DPR uses dense representations and demonstrates significant improvements over traditional sparse retrieval methods.}
}

@article{izacard2021leveraging,
  title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
  author={Izacard, Gautier and Grave, Edouard},
  journal={arXiv preprint arXiv:2007.01282},
  year={2021},
  url={https://arxiv.org/abs/2007.01282},
  abstract={We introduce a new approach that leverages passage retrieval combined with generative models for open-domain question answering. Our method, Fusion-in-Decoder, integrates retrieved passages into the generation process, achieving state-of-the-art results on several benchmarks.}
}

@article{wang2022gptj,
  title={GPT-J: An Open-Source Autoregressive Language Model},
  author={Wang, Ben and Komatsuzaki, Aran},
  year={2022},
  url={https://arxiv.org/abs/2106.08703},
  abstract={GPT-J is a 6B parameter open-source autoregressive language model trained on the Pile dataset. It serves as a foundation for various NLP tasks and allows for fine-tuning and deployment in custom applications, including RAG-based chatbots.}
}

@article{gudibande2021false,
  title={False Negatives in Retrieval-Based Chatbots},
  author={Gudibande, Siddharth and Khazaeni, Yasaman and et al.},
  journal={arXiv preprint arXiv:2104.07669},
  year={2021},
  url={https://arxiv.org/abs/2104.07669},
  abstract={This paper examines the issue of false negatives in retrieval-based chatbots and proposes methods to mitigate them. By improving retrieval accuracy, the effectiveness of RAG-based systems can be enhanced, leading to better user experiences.}
}

@article{sun2021survey,
  title={A Survey of Optimization Methods for Deep Learning Models Training},
  author={Sun, Shuguang and Cao, Zhiwei and Zhu, Hong and Zhao, Jinhui},
  journal={Journal of Systems Architecture},
  volume={117},
  pages={102112},
  year={2021},
  doi={10.1016/j.sysarc.2021.102112},
  abstract={This survey provides an overview of optimization methods for training deep learning models. It discusses various techniques that can be applied to improve training efficiency and model performance, which is relevant for developing scalable RAG-based chatbots.}
}

@article{rajpurkar2018know,
  title={Know What You Don't Know: Unanswerable Questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  pages={784--789},
  year={2018},
  doi={10.18653/v1/P18-2124},
  abstract={The paper introduces SQuAD 2.0, which combines answerable and unanswerable questions to test the ability of systems to know when they don't know the answer. This is crucial for building reliable RAG-based chatbots that can handle uncertain inputs gracefully.}
}

@article{gao2021rtd,
  title={RethinkDenoising: Efficient and Accurate Training of Large Transformer Models with Denoising Objectives},
  author={Gao, Tianyu and Liu, Xiao and Zheng, Shunzhong and Zhu, Hao and Li, Pengcheng},
  journal={arXiv preprint arXiv:2106.16241},
  year={2021},
  url={https://arxiv.org/abs/2106.16241},
  abstract={The authors propose an efficient method for training large transformer models using denoising objectives. This approach reduces computational costs and improves performance, which is beneficial for scalable deployment of RAG-based systems.}
}

@inproceedings{feng2020survey,
  title={A Survey of Deep Learning Approaches for Document Retrieval},
  author={Feng, Yida and Yang, Ziqi and et al.},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  pages={2107--2120},
  year={2020},
  doi={10.18653/v1/2020.emnlp-main.166},
  abstract={This survey reviews deep learning methods for document retrieval, highlighting advancements and challenges. Understanding these methods is essential for improving the retrieval component of RAG-based chatbots.}
}

@article{liu2023chatgpt,
  title={Is ChatGPT A Good Translator? A Preliminary Study},author={Liu, Yang and Zhou, Ming and others},
  journal={arXiv preprint arXiv:2301.08745},
  year={2023},
  url={https://arxiv.org/abs/2301.08745},
  abstract={The paper evaluates ChatGPT's performance in machine translation tasks, discussing its strengths and limitations. Insights from this study can inform the development of RAG-based chatbots regarding language understanding and generation capabilities.}
}

@article{zhang2021knn,
  title={kNN-LM: Improving Language Models with Nearest Neighbor Retrieval},
  author={Khandelwal, Urvashi and Fan, Angela and Jurafsky, Dan and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={727--742},
  year={2021},
  doi={10.1162/tacl_a_00393},
  abstract={kNN-LM augments neural language models with a k-nearest neighbors retrieval mechanism, improving performance on various language modeling tasks. This approach aligns with the RAG methodology by integrating retrieval into generation processes.}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023},
  url={https://arxiv.org/abs/2302.13971},
  abstract={LLaMA is a collection of foundation language models ranging from 7B to 65B parameters. These models are open-source and designed to be efficient, providing a valuable resource for building custom RAG-based chatbots.}
}

@article{roller2021recipes,
  title={Recipes for Building an Open-Domain Chatbot},
  author={Roller, Stephen and Dinan, Emily and Goyal, Naman and Ju, Da and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Shuster, Kurt and Smith, Eric M. and others},
  journal={arXiv preprint arXiv:2004.13637},
  year={2021},
  url={https://arxiv.org/abs/2004.13637},
  abstract={The paper provides practical guidelines and methods for building open-domain chatbots, discussing model architectures, training procedures, and evaluation metrics. These insights are valuable for designing and deploying scalable RAG-based chatbots.}
}

@inproceedings{su2022one,
  title={One Teacher is Enough? Pre-trained Language Model Distillation for Neural Machine Translation},
  author={Su, Jinhua and Ren, Dongdong and others},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  pages={136--146},
  year={2022},
  doi={10.18653/v1/2022.acl-long.13},
  abstract={The authors explore distillation methods for transferring knowledge from pre-trained language models to neural machine translation models, aiming to improve efficiency without significant loss in performance. This is relevant for optimizing models used in RAG-based systems.}
}

@article{yang2020model,
  title={Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System},
  author={Yang, Li and Wang, Yan and others},
  journal={IEEE Access},
  volume={8},
  pages={173334--173347},
  year={2020},
  doi={10.1109/ACCESS.2020.3025568},
  abstract={The paper presents a model compression method using multi-teacher knowledge distillation to reduce the size of models in web-based question answering systems. This approach can enhance the scalability of RAG-based chatbots by reducing resource requirements.}
}